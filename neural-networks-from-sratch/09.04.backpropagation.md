# Backpropagation Algorithm Example

## **Parameters and Input:**

* Input: $X = A^{[0]} = \begin{bmatrix} 0.5 \\ -0.2 \end{bmatrix}$
* Layer 1 Weights & Biases:
    $W^{[1]} = \begin{bmatrix} 0.1 & 0.4 \\ -0.2 & 0.1 \\ 0.3 & -0.1 \end{bmatrix}$, $b^{[1]} = \begin{bmatrix} 0.1 \\ 0 \\ -0.1 \end{bmatrix}$
* Layer 2 Weights & Biases:
    $W^{[2]} = \begin{bmatrix} 0.2 & -0.1 & 0.3 \\ 0.1 & 0.4 & -0.2 \end{bmatrix}$, $b^{[2]} = \begin{bmatrix} 0.2 \\ -0.1 \end{bmatrix}$
* True Label: $Y = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ (Class 1 is the correct class)

## **1. Forward Pass:**

* **Layer 1:**
    $Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$
    $Z^{[1]} = \begin{bmatrix} 0.1 & 0.4 \\ -0.2 & 0.1 \\ 0.3 & -0.1 \end{bmatrix} \begin{bmatrix} 0.5 \\ -0.2 \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0 \\ -0.1 \end{bmatrix}$
    $Z^{[1]} = \begin{bmatrix} (0.1*0.5 + 0.4*(-0.2)) \\ (-0.2*0.5 + 0.1*(-0.2)) \\ (0.3*0.5 + (-0.1)*(-0.2)) \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0 \\ -0.1 \end{bmatrix}$
    $Z^{[1]} = \begin{bmatrix} 0.05 - 0.08 \\ -0.10 - 0.02 \\ 0.15 + 0.02 \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0 \\ -0.1 \end{bmatrix} = \begin{bmatrix} -0.03 \\ -0.12 \\ 0.17 \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0 \\ -0.1 \end{bmatrix} = \begin{bmatrix} 0.07 \\ -0.12 \\ 0.07 \end{bmatrix}$

    $A^{[1]} = g_{relu}(Z^{[1]}) = \text{max}(0, Z^{[1]})$
    $A^{[1]} = \begin{bmatrix} \text{max}(0, 0.07) \\ \text{max}(0, -0.12) \\ \text{max}(0, 0.07) \end{bmatrix} = \begin{bmatrix} 0.07 \\ 0 \\ 0.07 \end{bmatrix}$

* **Layer 2 (Output):**
    $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$
    $Z^{[2]} = \begin{bmatrix} 0.2 & -0.1 & 0.3 \\ 0.1 & 0.4 & -0.2 \end{bmatrix} \begin{bmatrix} 0.07 \\ 0 \\ 0.07 \end{bmatrix} + \begin{bmatrix} 0.2 \\ -0.1 \end{bmatrix}$
    $Z^{[2]} = \begin{bmatrix} (0.2*0.07 + (-0.1)*0 + 0.3*0.07) \\ (0.1*0.07 + 0.4*0 + (-0.2)*0.07) \end{bmatrix} + \begin{bmatrix} 0.2 \\ -0.1 \end{bmatrix}$
    $Z^{[2]} = \begin{bmatrix} 0.014 + 0 + 0.021 \\ 0.007 + 0 - 0.014 \end{bmatrix} + \begin{bmatrix} 0.2 \\ -0.1 \end{bmatrix} = \begin{bmatrix} 0.035 \\ -0.007 \end{bmatrix} + \begin{bmatrix} 0.2 \\ -0.1 \end{bmatrix} = \begin{bmatrix} 0.235 \\ -0.107 \end{bmatrix}$

    $A^{[2]} = g_{softmax}(Z^{[2]})$
    Let $z_1 = 0.235, z_2 = -0.107$.
    $e^{z_1} \approx e^{0.235} \approx 1.2649$
    $e^{z_2} \approx e^{-0.107} \approx 0.8985$
    Sum: $e^{z_1} + e^{z_2} \approx 1.2649 + 0.8985 = 2.1634$
    $A^{[2]} = \hat{Y} = \begin{bmatrix} e^{z_1} / (e^{z_1} + e^{z_2}) \\ e^{z_2} / (e^{z_1} + e^{z_2}) \end{bmatrix} \approx \begin{bmatrix} 1.2649 / 2.1634 \\ 0.8985 / 2.1634 \end{bmatrix} \approx \begin{bmatrix} 0.5847 \\ 0.4153 \end{bmatrix}$

* **Loss Calculation (CCE):**
    $J = - \sum_{k=1}^{2} Y_k * \log(\hat{Y}_k)$ (Since m=1, no averaging needed yet)
    $J = - (0 * \log(0.5847) + 1 * \log(0.4153))$
    $J = - \log(0.4153) \approx -(-0.8787) \approx 0.8787$

## **2. Backward Pass:**

* **Layer 2 Gradients (`L=2`):**
    $\delta^{[2]} = \frac{\partial J}{\partial Z^{[2]}} = A^{[2]} - Y$ (Since m=1, 1/m=1)
    $\delta^{[2]} = \begin{bmatrix} 0.5847 \\ 0.4153 \end{bmatrix} - \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0.5847 \\ -0.5847 \end{bmatrix}$

    $\frac{\partial J}{\partial W^{[2]}} = \delta^{[2]} (A^{[1]})^T$
    $\frac{\partial J}{\partial W^{[2]}} = \begin{bmatrix} 0.5847 \\ -0.5847 \end{bmatrix} \begin{bmatrix} 0.07 & 0 & 0.07 \end{bmatrix}$
    $\frac{\partial J}{\partial W^{[2]}} = \begin{bmatrix} 0.5847*0.07 & 0.5847*0 & 0.5847*0.07 \\ -0.5847*0.07 & -0.5847*0 & -0.5847*0.07 \end{bmatrix} \approx \begin{bmatrix} 0.0409 & 0 & 0.0409 \\ -0.0409 & 0 & -0.0409 \end{bmatrix}$

    $\frac{\partial J}{\partial b^{[2]}} = \delta^{[2]}$ (Since m=1, sum reduces to the element itself)
    $\frac{\partial J}{\partial b^{[2]}} = \begin{bmatrix} 0.5847 \\ -0.5847 \end{bmatrix}$

* **Layer 1 Gradients (`l=1`):**
    First, find $\frac{\partial J}{\partial A^{[1]}} = (W^{[2]})^T \delta^{[2]}$
    $\frac{\partial J}{\partial A^{[1]}} = \begin{bmatrix} 0.2 & 0.1 \\ -0.1 & 0.4 \\ 0.3 & -0.2 \end{bmatrix} \begin{bmatrix} 0.5847 \\ -0.5847 \end{bmatrix}$
    $\frac{\partial J}{\partial A^{[1]}} = \begin{bmatrix} (0.2*0.5847 + 0.1*(-0.5847)) \\ (-0.1*0.5847 + 0.4*(-0.5847)) \\ (0.3*0.5847 + (-0.2)*(-0.5847)) \end{bmatrix} = \begin{bmatrix} 0.11694 - 0.05847 \\ -0.05847 - 0.23388 \\ 0.17541 + 0.11694 \end{bmatrix} \approx \begin{bmatrix} 0.05847 \\ -0.29235 \\ 0.29235 \end{bmatrix}$

    Next, find the derivative of ReLU activation $g'^{[1]}(Z^{[1]})$. Recall $Z^{[1]} = \begin{bmatrix} 0.07 \\ -0.12 \\ 0.07 \end{bmatrix}$.
    $g'^{[1]}(Z^{[1]}) = \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix}$ (since $Z^{[1]}_1 > 0$, $Z^{[1]}_2 \le 0$, $Z^{[1]}_3 > 0$)

    Now, calculate $\delta^{[1]} = \frac{\partial J}{\partial Z^{[1]}} = \frac{\partial J}{\partial A^{[1]}} * g'^{[1]}(Z^{[1]})$ (element-wise product)
    $\delta^{[1]} = \begin{bmatrix} 0.05847 \\ -0.29235 \\ 0.29235 \end{bmatrix} * \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0.05847 * 1 \\ -0.29235 * 0 \\ 0.29235 * 1 \end{bmatrix} = \begin{bmatrix} 0.05847 \\ 0 \\ 0.29235 \end{bmatrix}$

    Finally, calculate gradients for $W^{[1]}$ and $b^{[1]}$:
    $\frac{\partial J}{\partial W^{[1]}} = \delta^{[1]} (A^{[0]})^T$
    $\frac{\partial J}{\partial W^{[1]}} = \begin{bmatrix} 0.05847 \\ 0 \\ 0.29235 \end{bmatrix} \begin{bmatrix} 0.5 & -0.2 \end{bmatrix}$
    $\frac{\partial J}{\partial W^{[1]}} = \begin{bmatrix} 0.05847*0.5 & 0.05847*(-0.2) \\ 0*0.5 & 0*(-0.2) \\ 0.29235*0.5 & 0.29235*(-0.2) \end{bmatrix} \approx \begin{bmatrix} 0.0292 & -0.0117 \\ 0 & 0 \\ 0.1462 & -0.0585 \end{bmatrix}$

    $\frac{\partial J}{\partial b^{[1]}} = \delta^{[1]}$
    $\frac{\partial J}{\partial b^{[1]}} = \begin{bmatrix} 0.05847 \\ 0 \\ 0.29235 \end{bmatrix}$

## **Summary of Gradients (for m=1):**

* $\frac{\partial J}{\partial W^{[2]}} \approx \begin{bmatrix} 0.0409 & 0 & 0.0409 \\ -0.0409 & 0 & -0.0409 \end{bmatrix}$
* $\frac{\partial J}{\partial b^{[2]}} \approx \begin{bmatrix} 0.5847 \\ -0.5847 \end{bmatrix}$
* $\frac{\partial J}{\partial W^{[1]}} \approx \begin{bmatrix} 0.0292 & -0.0117 \\ 0 & 0 \\ 0.1462 & -0.0585 \end{bmatrix}$
* $\frac{\partial J}{\partial b^{[1]}} \approx \begin{bmatrix} 0.05847 \\ 0 \\ 0.29235 \end{bmatrix}$