{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36750ed5",
   "metadata": {},
   "source": [
    "# Bayesian Model Class Selection\n",
    "\n",
    "### The Simple Idea: Picking the Best Tool for the Job\n",
    "\n",
    "Imagine you have a messy room (your observed **data**, $D$), and you have several different cleaning tools (different **model classes**, $M_1, M_2, \\ldots, M_{N_M}$). Each tool represents a different way to explain or \"clean up\" the mess.\n",
    "\n",
    "*   **Model Class ($M_j$):** A specific type of explanation. For example, \"the mess was made by a cat\" ($M_1$), or \"the mess was made by a toddler\" ($M_2$), or \"it's just accumulated dust\" ($M_3$).\n",
    "*   **Data ($D$):** The actual observations. For example, \"knocked-over vase, muddy paw prints, ball of yarn.\"\n",
    "*   **The Set of All Considered Models ($\\mathfrak{M}$):** The collection of all tools you're considering: $\\mathfrak{M} = \\{M_1, M_2, M_3\\}$.\n",
    "\n",
    "**The Goal:** You want to figure out which tool (model class) is the most likely explanation for the messy room (data), given what you see and any prior beliefs you have about how likely each tool is to be the cause.\n",
    "\n",
    "Bayesian model selection provides a formal way to:\n",
    "1.  Quantify how well each model class explains the data.\n",
    "2.  Incorporate any prior beliefs about the models.\n",
    "3.  Calculate the updated probability (posterior probability) of each model class being the \"true\" one, after observing the data.\n",
    "\n",
    "You then pick the model class with the highest posterior probability, or you might consider a few top contenders.\n",
    "\n",
    "### Deepening into the Details\n",
    "\n",
    "The core of Bayesian model class selection is Bayes' theorem, applied at the level of model classes. We want to calculate the **posterior probability of a model class $M_j$ given the data $D$ and the set of considered model classes $\\mathfrak{M}$**, denoted as $P(M_j | D, \\mathfrak{M})$.\n",
    "\n",
    "The formula, as shown in your image (Equation 1.31), is:\n",
    "\n",
    "$P(M_j | D, \\mathfrak{M}) = \\frac{p(D | M_j) P(M_j | \\mathfrak{M})}{\\sum_{i=1}^{N_M} p(D | M_i) P(M_i | \\mathfrak{M})}$\n",
    "\n",
    "Let's break down each term:\n",
    "\n",
    "1.  **$P(M_j | D, \\mathfrak{M})$: Posterior Probability of Model $M_j$**\n",
    "    *   This is what we want to calculate. It represents our belief in model $M_j$ being the best explanation for the data $D$, *after* we have seen the data, considering it's one of the models in the set $\\mathfrak{M}$.\n",
    "    *   The model class with the highest posterior probability is considered the most plausible.\n",
    "\n",
    "2.  **$p(D | M_j)$: Marginal Likelihood (or Evidence) for Model $M_j$**\n",
    "    *   This is often the most crucial and computationally challenging term.\n",
    "    *   It quantifies how well model $M_j$ predicts the observed data $D$, on average, over all possible parameter values within that model.\n",
    "    *   Mathematically, if model $M_j$ has parameters $\\theta_j$, then the marginal likelihood is calculated by integrating (or marginalizing) out these parameters:\n",
    "        $p(D | M_j) = \\int p(D | \\theta_j, M_j) p(\\theta_j | M_j) d\\theta_j$\n",
    "        *   $p(D | \\theta_j, M_j)$ is the standard likelihood of the data given specific parameters $\\theta_j$ under model $M_j$.\n",
    "        *   $p(\\theta_j | M_j)$ is the prior probability distribution of the parameters $\\theta_j$ for model $M_j$.\n",
    "    *   The marginal likelihood naturally penalizes model complexity (an \"Occam's Razor\" effect). A more complex model (with more parameters or wider parameter priors) has to spread its predictive power over a larger parameter space. To achieve a high marginal likelihood, it must provide a *significantly* better fit to the data to overcome this dilution.\n",
    "\n",
    "3.  **$P(M_j | \\mathfrak{M})$: Prior Probability of Model $M_j$**\n",
    "    *   This represents our belief in model $M_j$ being the true model *before* observing any data, relative to the other models in the set $\\mathfrak{M}$.\n",
    "    *   If we have no reason to prefer one model over another initially, we often assign a uniform prior: $P(M_j | \\mathfrak{M}) = 1/N_M$ for all $j$.\n",
    "    *   These priors must sum to 1 over all models in $\\mathfrak{M}$: $\\sum_{j=1}^{N_M} P(M_j | \\mathfrak{M}) = 1$.\n",
    "\n",
    "4.  **$\\sum_{i=1}^{N_M} p(D | M_i) P(M_i | \\mathfrak{M})$: Normalization Constant (or Total Evidence for $\\mathfrak{M}$)**\n",
    "    *   This is the sum of the product of the marginal likelihood and prior probability for all models being considered.\n",
    "    *   It ensures that the posterior probabilities $P(M_j | D, \\mathfrak{M})$ sum to 1 over all $j$:\n",
    "        $\\sum_{j=1}^{N_M} P(M_j | D, \\mathfrak{M}) = 1$.\n",
    "    *   It represents the overall probability of observing the data $D$ given the entire set of models $\\mathfrak{M}$ and their priors.\n",
    "\n",
    "**In essence:** The posterior probability of a model is proportional to how well it explains the data (its marginal likelihood) multiplied by how much we believed in it beforehand (its prior probability). We then normalize these values across all considered models so they sum to one.\n",
    "\n",
    "**Key Takeaway from the Text:**\n",
    "The text emphasizes that models are approximations (\"the model itself may not necessarily reproduce the observed system, but it is just an approximation\"). Bayesian model selection helps us rank the \"relative performance\" of these candidate model classes in reproducing the data, providing \"information about the relative extent of support\" for each model.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code Example\n",
    "\n",
    "Let's create a synthetic example. We'll generate data from a known underlying process (e.g., a quadratic function with noise) and then try to select between two candidate models: a linear model and a quadratic model.\n",
    "\n",
    "For simplicity and to avoid complex numerical integration for the marginal likelihood $p(D|M_j)$, we'll use the **Bayesian Information Criterion (BIC)** as an approximation. The BIC for a model $M$ is given by:\n",
    "\n",
    "$BIC = k \\ln(n) - 2 \\ln(\\hat{L})$\n",
    "\n",
    "where:\n",
    "*   $n$ is the number of data points.\n",
    "*   $k$ is the number of parameters in the model.\n",
    "*   $\\hat{L}$ is the maximized value of the likelihood function for the model (i.e., $p(D|\\hat{\\theta}_{MLE}, M)$).\n",
    "\n",
    "The log marginal likelihood can be approximated by:\n",
    "$\\ln p(D|M) \\approx \\ln(\\hat{L}) - \\frac{k}{2} \\ln(n) = -0.5 \\times BIC$\n",
    "So, $p(D|M) \\approx \\exp(-0.5 \\times BIC)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644cf59",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
