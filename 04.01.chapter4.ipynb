{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6173db94",
   "metadata": {},
   "source": [
    "# Library load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7eb72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c2c4d",
   "metadata": {},
   "source": [
    "Neural net layer + activation functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997c4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Layer_Dense class with activation function support\n",
    "class Layer_DenseNumpy:\n",
    "    def __init__(self, n_inputs, n_neurons, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize weights, biases, and activation function for the layer.\n",
    "        :param n_inputs: Number of inputs to the layer\n",
    "        :param n_neurons: Number of neurons in the layer\n",
    "        :param activation: Activation function (e.g., relu, sigmoid, tanh, softmax)\n",
    "        \"\"\"\n",
    "        # NumPy version\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward_prop(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass for the layer.\n",
    "        :param inputs: Input data\n",
    "        \"\"\"\n",
    "        self.output_raw = inputs @ self.weights + self.biases\n",
    "        self.output = self.activation(self.output_raw) if self.activation else self.output_raw\n",
    "\n",
    "class Layer_DenseTorch:\n",
    "    def __init__(self, n_inputs, n_neurons, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize weights, biases, and activation function for the layer.\n",
    "        :param n_inputs: Number of inputs to the layer\n",
    "        :param n_neurons: Number of neurons in the layer\n",
    "        :param activation: Activation function (e.g., relu, sigmoid, tanh, softmax)\n",
    "        \"\"\"\n",
    "        # PyTorch version\n",
    "        # Check if CUDA is available and set device accordingly\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.weights = 0.01 * torch.randn(n_inputs, n_neurons, device=device)\n",
    "        self.biases = torch.zeros(1, n_neurons, device=device)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward_prop(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass for the layer.\n",
    "        :param inputs: Input data\n",
    "        \"\"\"\n",
    "        self.output_raw = inputs @ self.weights + self.biases\n",
    "        self.output = self.activation(self.output_raw) if self.activation else self.output_raw\n",
    "\n",
    "# Define activation functions as a dictionary for better modularity\n",
    "# CPU (numpy)\n",
    "activation_functions = {\n",
    "    \"linear\": lambda x: x,\n",
    "    \"relu\": lambda x: np.maximum(0, x),\n",
    "    \"sigmoid\": lambda x: 1 / (1 + np.exp(-x)),\n",
    "    \"tanh\": lambda x: np.tanh(x),\n",
    "    \"softmax\": lambda x: np.exp(x - np.max(x, axis=1, keepdims=True)) / np.sum(np.exp(x - np.max(x, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "}\n",
    "\n",
    "# GPU (PyTorch)\n",
    "activation_functions_torch = {\n",
    "    \"linear\": lambda x: x,\n",
    "    \"relu\": lambda x: F.relu(x),\n",
    "    \"sigmoid\": lambda x: torch.sigmoid(x),\n",
    "    \"tanh\": lambda x: torch.tanh(x),\n",
    "    \"softmax\": lambda x: F.softmax(x, dim=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2690653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> float32 uint8 (300, 2) (300,)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "print(type(X), type(y), X.dtype, y.dtype, X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868a702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 2])\n",
      "2\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "X_torch = torch.tensor(X, dtype=torch.float32).cuda()\n",
    "y_torch = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "\n",
    "print(X_torch.shape)\n",
    "print(X_torch.ndim)\n",
    "print(X_torch.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1308414",
   "metadata": {},
   "source": [
    "Forward pass with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d96850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_DenseTorch(2, 3, activation=activation_functions_torch[\"relu\"])\n",
    "dense2 = Layer_DenseTorch(3, 3, activation=activation_functions_torch[\"softmax\"])\n",
    "dense3 = Layer_DenseTorch(3, 1, activation=activation_functions_torch[\"linear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054507da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1.forward_prop(X_torch)\n",
    "dense2.forward_prop(dense1.output)\n",
    "dense3.forward_prop(dense2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4dba3",
   "metadata": {},
   "source": [
    "Forward pass outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56aa89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense1 output shape: torch.Size([300, 3])\n",
      "Dense2 output shape: torch.Size([300, 3])\n",
      "Dense3 output shape: torch.Size([300, 1])\n",
      "\n",
      "Dense1 output: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 8.0048e-05, 0.0000e+00],\n",
      "        [7.6293e-05, 1.0926e-04, 0.0000e+00]], device='cuda:0')\n",
      "Dense2 output: tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "Dense3 output: tensor([[0.0070],\n",
      "        [0.0070],\n",
      "        [0.0070]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('Dense1 output shape:', dense1.output.shape)  # (100, 3)\n",
    "print('Dense2 output shape:', dense2.output.shape)  # (100, 3)\n",
    "print('Dense3 output shape:', dense3.output.shape)  # (100, 1)\n",
    "print('\\nDense1 output:', dense1.output[:3])  # First 3 samples\n",
    "print('Dense2 output:', dense2.output[:3])  # First 3 samples\n",
    "print('Dense3 output:', dense3.output[:3])  # First 3 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91e046",
   "metadata": {},
   "source": [
    "La tercera capa de la red neuronal es coherente en cuanto dimensiones pero no tiene sentido meterla aquí. El output de la 2ª capa es 1/3 para cada uno de las muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a761217",
   "metadata": {},
   "source": [
    "Our example model is currently random. To remedy this, we need a way to\n",
    "calculate how wrong the neural network is at current predictions and begin adjusting weights\n",
    "and biases to decrease error over time. Thus, our next step is to quantify how wrong the model is\n",
    "through what’s defined as a ​loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c7794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
